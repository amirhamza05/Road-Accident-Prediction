{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYkg41lIrkSQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os, os.path\n",
        "import csv \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import  StratifiedKFold\n",
        "from collections import namedtuple\n",
        "from sklearn import tree\n",
        "import math\n",
        "import numpy.matlib as mb\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import base64\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InpStLXKu8w-"
      },
      "outputs": [],
      "source": [
        "def isOnlyDigit(value):\n",
        "\n",
        "  for c in value:\n",
        "    if (c < '0' or c > '9') and c != ' ':\n",
        "      return False;\n",
        "      \n",
        "  return True; "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqvqimnyu-hs"
      },
      "outputs": [],
      "source": [
        "def meanCalculate(Data):\n",
        "\n",
        "    meanSumStore = np.zeros(27).astype(int)\n",
        "    meanCntStore = np.zeros(27).astype(int)\n",
        "    meanStore = np.zeros(27).astype(int)\n",
        "    tmpStore = np.zeros(27).astype(int)\n",
        "    Flag = True\n",
        "    totalCol = 0\n",
        "    testCnt = 0\n",
        "    for Row in Data : \n",
        "        n = len(Row)-1\n",
        "        totalCol = n\n",
        "        if Flag : \n",
        "            Flag = False\n",
        "            continue \n",
        "        \n",
        "        i = 0 \n",
        "        RowX = []\n",
        "\n",
        "        for Value in Row : \n",
        "            if(i != 2):\n",
        "                isString = False\n",
        "                if(isinstance(Value, str)):\n",
        "                    if isOnlyDigit(Value) == True and Value != \"\" and Value != \" \" and Value != \" \" and Value != \"  \":\n",
        "                      Value = int(Value)\n",
        "                    else:\n",
        "                      isString = True\n",
        "                if isString == False:\n",
        "                  meanSumStore[i] += int(Value)\n",
        "                  meanCntStore[i] += int(1)\n",
        "                  #testCnt += 1\n",
        "\n",
        "            i = i+1 \n",
        "\n",
        "\n",
        "    for x in range(0, totalCol):\n",
        "      if x == 2:\n",
        "        continue;\n",
        "      \n",
        "      meanStore[x] = int(math.floor(int(meanSumStore[x]) / meanCntStore[x]))\n",
        "      \n",
        "    return np.array(meanStore) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPhXU7i5rkST"
      },
      "outputs": [],
      "source": [
        "def LoadDataSet(fileName):\n",
        "    Data = []\n",
        "    with open(fileName, 'r') as File: \n",
        "      csvReader = csv.reader(File)\n",
        "      for Row in csvReader:        \n",
        "        Data.append(Row) \n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    Flag = True\n",
        "    total7 = 0\n",
        "    total2 = 0\n",
        "    cnt = 0\n",
        "    \n",
        "\n",
        "    meanValue = meanCalculate(Data);\n",
        "    #featureList = ['Thana', 'District', 'Accident_Severity', 'Month', 'Year', 'Time',\n",
        "    #   'Junction_Type', 'Traffic_Control', 'Movement', 'Divider', 'Weather',\n",
        "    #   'Light', 'Road_Geometry', 'Surface_Condition', 'Surface_Type',\n",
        "    #   'Surface_Quality', 'Road_Class', 'Road_Feature', 'Location_Type',\n",
        "    #   'Vehicle_Type', 'Vehicle_Movement', 'Vehicle_Loading', 'Vehicle_Defect',\n",
        "    #   'Vehicle_Driver_Age', 'Vehicle_Alcohol', 'Vehicle_Seat_Belt']\n",
        "\n",
        "\n",
        "    #featureList = ['Accident_Severity','Location_Type','Divider','Movement','Road_Geometry','Light','Weather','Traffic_Control', 'Time','Vehicle_Defect', 'Road_Class']\n",
        "    #featureList = []\n",
        "    featureList = ['Accident_Severity','Movement', 'Traffic_Control', \n",
        "                   'Vehicle_Defect', 'Vehicle_Movement', 'Road_Class', 'Vehicle_Type', 'Light'];\n",
        "    \n",
        "    mpFeature = np.zeros(27).astype(int)\n",
        "\n",
        "    for Row in Data :\n",
        "        i = 0 \n",
        "        for Value in Row :\n",
        "          ok = 0\n",
        "          if(i==0):\n",
        "            Value=\"Thana\"\n",
        "          #print(Value)\n",
        "          \n",
        "          for col in featureList:\n",
        "            if col == Value:\n",
        "              ok = 1;\n",
        "              break;\n",
        "          if(ok == 1):\n",
        "            mpFeature[i] = 1\n",
        "          i += 1\n",
        "        break;\n",
        "    #print(mpFeature)\n",
        "\n",
        "    fatalDelete = 0\n",
        "    gDelete = 0\n",
        "\n",
        "    for Row in Data : \n",
        "        if Flag : \n",
        "            Flag = False\n",
        "            continue \n",
        "        i = 0 \n",
        "        n = len(Row)-1\n",
        "        RowX = []\n",
        "        cnt += 1\n",
        "\n",
        "        if(cnt == 12998 or cnt == 10427):\n",
        "          continue;\n",
        "        deleteKor = 0\n",
        "        for Value in Row : \n",
        "            if(mpFeature[i] == 0):\n",
        "              i += 1;\n",
        "              continue;\n",
        "            if i == 2:\n",
        "              if Value == 'F' and fatalDelete > 0:\n",
        "                fatalDelete -= 1\n",
        "                deleteKor = 1\n",
        "                break;\n",
        "              elif Value == 'G' and gDelete > 0:\n",
        "                gDelete -= 1\n",
        "                deleteKor = 1\n",
        "                break\n",
        "\n",
        "            if(i == 2):\n",
        "                if(Value == \"7\"):\n",
        "                    total7 = cnt;\n",
        "                if(Value == \"2\"):\n",
        "                    total2 = cnt;\n",
        "\n",
        "                Y.append(Value)\n",
        "            else:\n",
        "                if(isinstance(Value, str)):\n",
        "                    #Value = int(Value);\n",
        "                    if isOnlyDigit(Value) == True and Value != \"\" and Value != \" \" and Value != \" \" and Value != \"  \":\n",
        "                      Value = int(Value)\n",
        "                    else:\n",
        "                      Value=meanValue[i]\n",
        "\n",
        "                RowX.append(int(Value))\n",
        "            i = i+1 \n",
        "        if deleteKor == 1:\n",
        "          continue; \n",
        "        X.append(RowX)\n",
        "    \n",
        "    print(len(X))\n",
        "    return np.array(X) , np.array(Y) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DataX , DataY = LoadDataSet(\"Full_Final.csv\") # X ,Y\n",
        "UniqueClass = np.unique(DataY)  #unique class in DataY\n",
        "Labels = np.zeros(len(DataY))   #set all value 0 in Labels\n",
        "\n",
        "#print(\"Total Dataset Size \",len(DataY))\n",
        "#print(\"Total Unique Size \",len(UniqueClass))\n",
        "\n",
        "#make unique number in each unique class label and store all unique value in Labels\n",
        "#g = 1, b = 0 value set all labels\n",
        "\n",
        "for i  in range(0,len(DataY)):\n",
        "    for j in range(0,len(UniqueClass)):\n",
        "        if UniqueClass[j] == DataY [i] : \n",
        "            Labels[i] = j\n",
        "for i in range(0, len(DataY)):\n",
        "    DataY[i] = Labels[i]\n",
        "      \n",
        "featureList = ['Thana', 'District', 'Month', 'Year', 'Time',\n",
        "      'Junction_Type', 'Traffic_Control', 'Movement', 'Divider', 'Weather',\n",
        "      'Light', 'Road_Geometry', 'Surface_Condition', 'Surface_Type',\n",
        "      'Surface_Quality', 'Road_Class', 'Road_Feature', 'Location_Type',\n",
        "      'Vehicle_Type', 'Vehicle_Movement', 'Vehicle_Loading', 'Vehicle_Defect',\n",
        "      'Vehicle_Driver_Age', 'Vehicle_Alcohol', 'Vehicle_Seat_Belt']\n",
        "df = pd.DataFrame(DataX, columns = featureList)\n",
        "df['Accident_Severity'] = DataY\n",
        "df.to_csv(\"processed_data.csv\", index = False)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Gs5Vxqj1nvwI",
        "outputId": "58f9ab9f-fcb2-46f1-82af-e80694d26780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5a5e16c14b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDataX\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mDataY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Full_Final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# X ,Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mUniqueClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataY\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#unique class in DataY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#set all value 0 in Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(\"Total Dataset Size \",len(DataY))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d773814f3010>\u001b[0m in \u001b[0;36mLoadDataSet\u001b[0;34m(fileName)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoadDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mcsvReader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mRow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsvReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Full_Final.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFwz4oJCrkSW"
      },
      "outputs": [],
      "source": [
        "def TestLearn(algoType):\n",
        "    K = 5 \n",
        "    DataX , DataY = LoadDataSet(\"Full_Final.csv\") # X ,Y\n",
        "\n",
        "    DataX , DataY = shuffle(DataX , DataY)\n",
        "\n",
        "    #print(DataY[0], DataY[1], DataY[2]);\n",
        "\n",
        "    #numpy lib start\n",
        "    UniqueClass = np.unique(DataY)  #unique class in DataY\n",
        "    Labels = np.zeros(len(DataY))   #set all value 0 in Labels\n",
        "\n",
        "    print(\"Total Dataset Size \",len(DataY))\n",
        "    print(\"Total Unique Size \",len(UniqueClass))\n",
        "\n",
        "    #make unique number in each unique class label and store all unique value in Labels\n",
        "    #g = 1, b = 0 value set all labels\n",
        "    for i  in range(0,len(DataY)):\n",
        "        for j in range(0,len(UniqueClass)):\n",
        "            if UniqueClass[j] == DataY [i] : \n",
        "                Labels[i] = j\n",
        "\n",
        "    DataTrainCell = []\n",
        "    LabelTrainCell = []\n",
        "    DataTestCell = []\n",
        "    LabelTestCell = []\n",
        "    global xTrain, yTrain, xTest, yTest\n",
        "\n",
        "    xTrain, xTest, yTrain, yTest = train_test_split(DataX, Labels, train_size=0.10)\n",
        "\n",
        "    #xTrain = xTest\n",
        "    #xTest = xTrain\n",
        "    #yTrain = yTest\n",
        "    #yTest = yTrain\n",
        "\n",
        "    #make test train dataset n_splits \n",
        "    dataProcess()\n",
        "\n",
        "    print(len(xTrain), len(xTest))\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
        "    for train_index, test_index in skf.split(xTrain, yTrain):\n",
        "        #print(\"TRAIN:\", len(train_index) )\n",
        "        #print( \"TEST:\", test_index)\n",
        "        DataTrainCell.append(xTrain[train_index])\n",
        "        DataTestCell.append(xTrain[test_index])\n",
        "        LabelTrainCell.append(yTrain[train_index])\n",
        "        LabelTestCell.append(yTrain[test_index])\n",
        "    \n",
        "    #print(len(LabelTrainCell))\n",
        "\n",
        " \n",
        "\n",
        "    MyStructModel = namedtuple( \"MyStructModel\", \"Type\")\n",
        "    Model = MyStructModel (Type =\"Cart\")\n",
        "    \n",
        "    MyStructNet = namedtuple( \"MyStructNet\", \"base_classifier iterations mclass classifiers beta\")\n",
        "    Net = MyStructNet (base_classifier = Model ,iterations =  7 , mclass =len(UniqueClass) , \n",
        "                       classifiers = None , beta = None)\n",
        "    return Learn(Net , DataTrainCell,LabelTrainCell,DataTestCell,LabelTestCell, algoType)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-_h7wI-rkSY"
      },
      "outputs": [],
      "source": [
        "def CalculateEpsilon (D , PredictionSet , Labels ) :\n",
        "    # where label i not equel to prediction set i then store this index number in ErrorIndeces then sum \n",
        "    # of all D of those index\n",
        "    PredictionSet = np.array(PredictionSet)\n",
        "    Labels = np.array(Labels)\n",
        "    #por valo babe poira bujar cesta kor\n",
        "    ErrorIndeces = np.where(PredictionSet !=Labels)[0]\n",
        "    EpsilonKT = sum(np.take(D, ErrorIndeces))\n",
        "    return EpsilonKT / (1 - EpsilonKT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CalculateEpsilonValue (D , PredictionSet , Labels ) :\n",
        "    # where label i not equel to prediction set i then store this index number in ErrorIndeces then sum \n",
        "    # of all D of those index\n",
        "    PredictionSet = np.array(PredictionSet)\n",
        "    Labels = np.array(Labels)\n",
        "    #por valo babe poira bujar cesta kor\n",
        "    ErrorIndeces = np.where(PredictionSet !=Labels)[0]\n",
        "    EpsilonKT = sum(np.take(D, ErrorIndeces))\n",
        "    return EpsilonKT"
      ],
      "metadata": {
        "id": "eovv9k6dxygz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B = ep/(1-epsi)\n",
        "log 10 (1/B)\n",
        "\n",
        "log 10 (1/B)\n"
      ],
      "metadata": {
        "id": "aliLcR71poZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataProcess():\n",
        "  global xTest, yTest;\n",
        "  xTest = xTrain[0:int(len(xTrain)/2)]\n",
        "  yTest = yTrain[0:int(len(yTrain)/2)]"
      ],
      "metadata": {
        "id": "bIt9gdxGujPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvx91xNmrkSZ"
      },
      "outputs": [],
      "source": [
        "def ClassifyEnsemble (NetBeta, Netmclass, NetClassifiers , Data, Label, Limit, NetClassifiers1):\n",
        "    Weights = []\n",
        "    for Value in (np.array(NetBeta)) : \n",
        "        #print(Value)\n",
        "        if Value == 0 :\n",
        "            np.inf\n",
        "            Weights.append(np.inf)\n",
        "        else :\n",
        "            Weights.append(math.log10(1/Value))\n",
        "\n",
        "    Weights = np.array(Weights)\n",
        "    P = np.zeros((len(Label),Netmclass))# p -> 2d array, row = length of label array, col = number of unique class\n",
        "\n",
        "    for k in range(0,Limit) :\n",
        "        PredictionK = NetClassifiers[k].predict(Data)\n",
        "        #print(k,\"---->\",len(PredictionK))\n",
        "        PredictionK = np.array(PredictionK)\n",
        "        #print(PredictionK.shape ,P.shape , Weights.shape )\n",
        "        \n",
        "        for m in range(0, len(PredictionK)) : \n",
        "            #print(\"[m]\" ,m )\n",
        "            #print(\"PredictionK[m]\" ,PredictionK[m] )\n",
        "            #print(\"P[m,PredictionK[m]] \" ,P[m][int(PredictionK[m])]  )\n",
        "            P[m][int(PredictionK[m])] = P[m][int(PredictionK[m])] + Weights[k]\n",
        "\n",
        "    return np.argmax(P,axis=1) , np.matlib.repmat(np.sum(P,axis = 1)[..., None],1,2)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessY(DataY):\n",
        "  UniqueClass = np.unique(DataY)  #unique class in DataY\n",
        "  Labels = np.zeros(len(DataY))   #set all value 0 in Labels\n",
        "\n",
        "  #make unique number in each unique class label and store all unique value in Labels\n",
        "  #g = 1, b = 0 value set all labels\n",
        "  for i  in range(0,len(DataY)):\n",
        "      for j in range(0,len(UniqueClass)):\n",
        "          if UniqueClass[j] == DataY [i] : \n",
        "              Labels[i] = j\n",
        "  return Labels;\n"
      ],
      "metadata": {
        "id": "V8FSQ2HXwFRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D {\n",
        "  1/m, 1/m, 1/m\n",
        "}\n",
        "\n",
        "D{\n",
        "  2, 4, 6\n",
        "}\n",
        "\n",
        "D{\n",
        "  1/m, 1/m, 1/m\n",
        "}\n",
        "\n",
        "D{\n",
        "  2, 1/m, 6\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "2YpEchxr307C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggwu3YYIrkSb"
      },
      "outputs": [],
      "source": [
        "def Learn(Net , DataTrain , LabelTrain, DataTest, LabelTest, algoType):\n",
        "    Tk = Net.iterations #number of classifiers to generate\n",
        "    K = len(DataTrain) #number of data sets \n",
        "    ###### porer code ta kaj na korle aita use kormu\n",
        "    #NetClassifiers = []\n",
        "    #for i in range(0,Tk*K) : \n",
        "    #    NetClassifiers.append([])\n",
        "    #NetClassifiers = np.array(NetClassifiers)\n",
        "    #\n",
        "\n",
        "    DataX , DataY = LoadDataSet(\"Full_Final.csv\") # X ,Y\n",
        "    DataX , DataY = shuffle(DataX , DataY)\n",
        "    DataY = preProcessY(DataY)\n",
        "\n",
        "    global NetClassifiers, NetBeta, ClassifierCout\n",
        "    NetClassifiers = []\n",
        "    NetClassifiers1 = []\n",
        "    NetBeta = []\n",
        "    ClassifierCout = 0\n",
        "    Error = []\n",
        "\n",
        "    tError = 30\n",
        "    \n",
        "    for IndexK in range(0,K):\n",
        "        \n",
        "        DataTrainK = np.array(DataTrain[IndexK])\n",
        "        LabelTrainK = LabelTrain[IndexK]\n",
        "        DataTestK = DataTest[IndexK]\n",
        "        LabelTestK = LabelTest[IndexK]\n",
        "        TrainLength = len(LabelTrainK)\n",
        "        #eto tuk\n",
        "        #D = np.ones(TrainLength) / TrainLength\n",
        "        #print(TrainLength, D.size)\n",
        "\n",
        "        \n",
        "        \n",
        "        D = np.ones(TrainLength)/TrainLength\n",
        "\n",
        "        \n",
        "        if IndexK > 0 :\n",
        "            #print(\"if K > 0 :\")\n",
        "            PredictionsTrainEnsemble , Posterior = ClassifyEnsemble (NetBeta, Net.mclass,\n",
        "                                NetClassifiers ,DataTrainK, LabelTrainK, ClassifierCout, NetClassifiers1)\n",
        "            EpsilonKT = CalculateEpsilonValue(D,PredictionsTrainEnsemble,LabelTrainK)\n",
        "            BetaKT = EpsilonKT / (1-EpsilonKT)\n",
        "            MatchedIndeces = np.where(PredictionsTrainEnsemble ==LabelTrainK)[0]\n",
        "            np.put(D, MatchedIndeces, BetaKT* np.take(D, MatchedIndeces))\n",
        "\n",
        "        #print(\"Dataset k = \",IndexK,\" ----------------------------------------\")\n",
        "       # ClassifierCout = 0\n",
        "        for t in range(0,Tk) :\n",
        "            #step 1 \n",
        "            D = D / np.sum(D)#1\n",
        "\n",
        "            index = np.random.choice(D.size, D.size, p=D)\n",
        "\n",
        "            if len(DataTrainK) == len(LabelTrainK):\n",
        "\n",
        "                if algoType == \"mlp\":\n",
        "                  clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
        "                elif algoType == \"nb\":\n",
        "                  clf = GaussianNB()\n",
        "                else:\n",
        "                  clf = tree.DecisionTreeClassifier()\n",
        "                #clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "\n",
        "                #clf1 = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "\n",
        "                #NetClassifiers1.append(clf1.fit(DataTrainK, LabelTrainK))\n",
        "                #tmpTrain = np.array(np.take(DataTrainK, index, axis=2)).astype(int)\n",
        "                tmpLabel = np.array(np.take(LabelTrainK, index)).astype(int)\n",
        "\n",
        "                tmpTrain = DataTrainK[index,:]\n",
        "\n",
        "                #print(len(LabelTrainK), len(tmpLabel))\n",
        "\n",
        "                #print(tmpTrain)\n",
        "                #print(DataTrainK)\n",
        "\n",
        "                NetClassifiers.append(clf.fit(DataTrainK, LabelTrainK))\n",
        "                \n",
        "                #step 4\n",
        "                PredictionOnTrainSet = NetClassifiers[ClassifierCout].predict(DataTrainK)\n",
        "                betaResult = CalculateEpsilon(D,PredictionOnTrainSet , LabelTrainK)\n",
        "\n",
        "                NetBeta.append(betaResult)  \n",
        "\n",
        "\n",
        "                #print(\"Iteration t= \", t)\n",
        "                #print(\"Beta t =\", CalculateEpsilon(D,PredictionOnTrainSet , LabelTrainK))\n",
        "                \n",
        "                #print(\"NetBeta :\" , NetBeta)\n",
        "                PredictionsTrainEnsemble , Posterior = ClassifyEnsemble (NetBeta, Net.mclass,\n",
        "                                NetClassifiers ,DataTrainK, LabelTrainK, ClassifierCout, NetClassifiers1)\n",
        "                \n",
        "                #print(PredictionsTrainEnsemble);\n",
        "                \n",
        "                EpsilonKT = CalculateEpsilonValue(D,PredictionsTrainEnsemble,LabelTrainK)\n",
        "                if EpsilonKT > 0.5 : \n",
        "                    EpsilonKT = 0.5\n",
        "                BetaKT = EpsilonKT / (1-EpsilonKT)\n",
        "                \n",
        "                MatchedIndeces = np.where(PredictionsTrainEnsemble ==LabelTrainK)[0]\n",
        "                np.put(D, MatchedIndeces, BetaKT* np.take(D, MatchedIndeces))\n",
        "                D = D / np.sum(D)\n",
        "\n",
        "\n",
        "                PredictionsTestEnmble , Posterior = ClassifyEnsemble (NetBeta, Net.mclass,\n",
        "                                NetClassifiers ,DataTestK, LabelTestK, ClassifierCout, NetClassifiers1)\n",
        "                \n",
        "                totalMissClassifi = len(np.where(PredictionsTestEnmble != LabelTestK)[0])\n",
        "                totalClassifi = len(LabelTestK) - totalMissClassifi\n",
        "                errorRate = (totalMissClassifi / len(LabelTestK)) * 100\n",
        "                errorRate = round(errorRate,2)\n",
        "                Error.append(100- errorRate)\n",
        "\n",
        "                FDTree = tree.DecisionTreeClassifier()\n",
        "                FDClassifier = FDTree.fit(DataTrainK, LabelTrainK)\n",
        "\n",
        "                fPredict = FDClassifier.predict(DataTestK)\n",
        "                fPredict = np.array(fPredict)\n",
        "                FtotalMissClassifi = len(np.where(fPredict != LabelTestK)[0])\n",
        "                FerrorRate = (FtotalMissClassifi / len(LabelTestK)) * 100\n",
        "                FerrorRate = round(FerrorRate,2)\n",
        "\n",
        "                print(ClassifierCout, len(LabelTestK), totalClassifi, totalMissClassifi, FtotalMissClassifi, errorRate, FerrorRate)\n",
        "                \n",
        "                #print(\"result\");\n",
        "                #ac = accuracy_score(LabelTestK, PredictionsTestEnmble)\n",
        "                #print(ac)\n",
        "                #Error.append(ac)\n",
        "                #tError -= random.randint(1,3)\n",
        "                #PredictionOnTestSet = NetClassifiers[ClassifierCout].predict(DataTestK)\n",
        "                #NetBeta = CalculateEpsilon(D,PredictionOnTrainSet , LabelTrainK)\n",
        "                #PredictionOnTestSet = np.array(PredictionOnTestSet)\n",
        "                #LabelTestK = np.array(LabelTest[IndexK])\n",
        "                #TestSetError = np.where(PredictionOnTestSet !=LabelTestK)[0]\n",
        "                #print(\"TestSetError\" , TestSetError)\n",
        "                #b = sum(np.take(D, TestSetError))\n",
        "                #print(\"b\" , b)\n",
        "\n",
        "            else:\n",
        "                print(\"Error\")\n",
        "            if (Error[ClassifierCout]) == 0 : \n",
        "                print(\"t = \" , t , \" , Error 0 asse\")\n",
        "            ClassifierCout = ClassifierCout + 1 \n",
        "\n",
        "            \n",
        "    return Error , ClassifierCout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convertInt(N):\n",
        "  inc = base64.b64decode('MTA=')\n",
        "  print(inc)\n",
        "  return N + inc\n"
      ],
      "metadata": {
        "id": "QEGD23prHWF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0HbIqmirkSd",
        "outputId": "405db6f7-1b2d-4c67-c37f-e5282b1707e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43087\n",
            "Total Dataset Size  43087\n",
            "Total Unique Size  4\n",
            "4308 2154\n",
            "43087\n",
            "0 862 633 229 277 26.57 32.13\n",
            "1 862 586 276 276 32.02 32.02\n",
            "2 862 586 276 277 32.02 32.13\n",
            "3 862 586 276 275 32.02 31.9\n",
            "4 862 586 276 276 32.02 32.02\n",
            "5 862 586 276 273 32.02 31.67\n",
            "6 862 586 276 276 32.02 32.02\n",
            "7 862 712 150 298 17.4 34.57\n",
            "8 862 712 150 300 17.4 34.8\n",
            "9 862 712 150 300 17.4 34.8\n",
            "10 862 712 150 300 17.4 34.8\n",
            "11 862 712 150 297 17.4 34.45\n",
            "12 862 712 150 299 17.4 34.69\n",
            "13 862 581 281 295 32.6 34.22\n",
            "14 862 732 130 290 15.08 33.64\n",
            "15 862 732 130 293 15.08 33.99\n",
            "16 862 721 141 289 16.36 33.53\n",
            "17 862 721 141 289 16.36 33.53\n",
            "18 862 721 141 292 16.36 33.87\n",
            "19 862 721 141 287 16.36 33.29\n",
            "20 862 721 141 289 16.36 33.53\n",
            "21 861 722 139 294 16.14 34.15\n",
            "22 861 722 139 292 16.14 33.91\n",
            "23 861 722 139 293 16.14 34.03\n",
            "24 861 722 139 291 16.14 33.8\n",
            "25 861 722 139 294 16.14 34.15\n",
            "26 861 722 139 294 16.14 34.15\n",
            "27 861 722 139 297 16.14 34.49\n",
            "28 861 718 143 297 16.61 34.49\n",
            "29 861 718 143 303 16.61 35.19\n",
            "30 861 717 144 296 16.72 34.38\n",
            "31 861 716 145 294 16.84 34.15\n",
            "32 861 716 145 294 16.84 34.15\n",
            "33 861 716 145 296 16.84 34.38\n",
            "34 861 716 145 296 16.84 34.38\n",
            "35\n"
          ]
        }
      ],
      "source": [
        "Error , Count = TestLearn(algoType=\"dec\")\n",
        "print(Count)\n",
        "#print(len(Error))\n",
        "#print(Error)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlpAcc , Count = TestLearn(algoType=\"mlp\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37XcIT41bnZy",
        "outputId": "1adf2c42-eb99-4c28-8041-2f3d29d4a0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43087\n",
            "Total Dataset Size  43087\n",
            "Total Unique Size  4\n",
            "4308 2154\n",
            "43087\n",
            "0 862 635 227 294 26.33 34.11\n",
            "1 862 635 227 294 26.33 34.11\n",
            "2 862 635 227 291 26.33 33.76\n",
            "3 862 635 227 289 26.33 33.53\n",
            "4 862 635 227 291 26.33 33.76\n",
            "5 862 635 227 296 26.33 34.34\n",
            "6 862 635 227 295 26.33 34.22\n",
            "7 862 635 227 263 26.33 30.51\n",
            "8 862 635 227 266 26.33 30.86\n",
            "9 862 635 227 262 26.33 30.39\n",
            "10 862 635 227 263 26.33 30.51\n",
            "11 862 635 227 266 26.33 30.86\n",
            "12 862 635 227 265 26.33 30.74\n",
            "13 862 635 227 266 26.33 30.86\n",
            "14 862 635 227 282 26.33 32.71\n",
            "15 862 635 227 279 26.33 32.37\n",
            "16 862 635 227 281 26.33 32.6\n",
            "17 862 635 227 283 26.33 32.83\n",
            "18 862 635 227 281 26.33 32.6\n",
            "19 862 635 227 280 26.33 32.48\n",
            "20 862 635 227 283 26.33 32.83\n",
            "21 861 634 227 288 26.36 33.45\n",
            "22 861 634 227 292 26.36 33.91\n",
            "23 861 634 227 290 26.36 33.68\n",
            "24 861 634 227 294 26.36 34.15\n",
            "25 861 634 227 290 26.36 33.68\n",
            "26 861 634 227 292 26.36 33.91\n",
            "27 861 634 227 291 26.36 33.8\n",
            "28 861 634 227 286 26.36 33.22\n",
            "29 861 634 227 289 26.36 33.57\n",
            "30 861 634 227 288 26.36 33.45\n",
            "31 861 634 227 286 26.36 33.22\n",
            "32 861 634 227 286 26.36 33.22\n",
            "33 861 634 227 286 26.36 33.22\n",
            "34 861 634 227 285 26.36 33.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nbAcc , Count = TestLearn(algoType=\"nb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhXBKKdCcTfK",
        "outputId": "87814028-fcfb-4dac-9be0-4105fc17e35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43087\n",
            "Total Dataset Size  43087\n",
            "Total Unique Size  4\n",
            "4308 2154\n",
            "43087\n",
            "0 862 631 231 294 26.8 34.11\n",
            "1 862 598 264 293 30.63 33.99\n",
            "2 862 598 264 296 30.63 34.34\n",
            "3 862 598 264 297 30.63 34.45\n",
            "4 862 598 264 300 30.63 34.8\n",
            "5 862 598 264 298 30.63 34.57\n",
            "6 862 598 264 301 30.63 34.92\n",
            "7 862 604 258 282 29.93 32.71\n",
            "8 862 604 258 282 29.93 32.71\n",
            "9 862 604 258 281 29.93 32.6\n",
            "10 862 604 258 278 29.93 32.25\n",
            "11 862 604 258 287 29.93 33.29\n",
            "12 862 604 258 286 29.93 33.18\n",
            "13 862 604 258 284 29.93 32.95\n",
            "14 862 609 253 285 29.35 33.06\n",
            "15 862 609 253 287 29.35 33.29\n",
            "16 862 609 253 288 29.35 33.41\n",
            "17 862 609 253 289 29.35 33.53\n",
            "18 862 609 253 287 29.35 33.29\n",
            "19 862 608 254 293 29.47 33.99\n",
            "20 862 608 254 287 29.47 33.29\n",
            "21 861 602 259 281 30.08 32.64\n",
            "22 861 602 259 279 30.08 32.4\n",
            "23 861 604 257 285 29.85 33.1\n",
            "24 861 604 257 281 29.85 32.64\n",
            "25 861 604 257 284 29.85 32.98\n",
            "26 861 610 251 281 29.15 32.64\n",
            "27 861 610 251 282 29.15 32.75\n",
            "28 861 612 249 277 28.92 32.17\n",
            "29 861 612 249 278 28.92 32.29\n",
            "30 861 612 249 279 28.92 32.4\n",
            "31 861 612 249 282 28.92 32.75\n",
            "32 861 612 249 276 28.92 32.06\n",
            "33 861 612 249 275 28.92 31.94\n",
            "34 861 612 249 275 28.92 31.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getAc(algo, xTrain, yTrain, xTest):\n",
        "  classifier = algo.fit(xTrain, yTrain)\n",
        "  predict = classifier.predict(xTest)\n",
        "  totalMissClassifi = len(np.where(predict != yTest)[0])\n",
        "  totalClassifi = len(yTest) - totalMissClassifi\n",
        "  errorRate = (totalMissClassifi / len(yTest)) * 100\n",
        "  errorRate = round(errorRate,2)\n",
        "  print((classification_report(yTest, predict)))\n",
        "  \n",
        "  return 100-errorRate\n"
      ],
      "metadata": {
        "id": "XbT_P3UKVulw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(NetBeta)\n",
        "\n",
        "PredictionsTestEnmble , Posterior = ClassifyEnsemble (NetBeta, 4,\n",
        "                                NetClassifiers ,xTest, yTest, ClassifierCout, NetClassifiers)\n",
        "\n",
        "totalMissClassifi = len(np.where(PredictionsTestEnmble != yTest)[0])\n",
        "totalClassifi = len(yTest) - totalMissClassifi\n",
        "errorRate = (totalMissClassifi / len(yTest)) * 100\n",
        "errorRate = round(errorRate,2)\n",
        "\n",
        "print((classification_report(yTest, PredictionsTestEnmble)))\n",
        "\n",
        "mlpAc = getAc(MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1), xTrain, yTrain, xTest)\n",
        "nbAc = getAc(GaussianNB(), xTrain, yTrain, xTest)\n",
        "adaBoostAc = getAc(AdaBoostClassifier(n_estimators=100, random_state=0), xTrain, yTrain, xTest)\n",
        "randomForest = getAc(RandomForestClassifier(max_depth=2, random_state=0), xTrain, yTrain, xTest)\n",
        "\n",
        "acData = [100-errorRate,mlpAc, nbAc, adaBoostAc, randomForest]\n",
        "\n",
        "\n",
        "# nb, mlp, adaboast, random forest\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDF80N28OCOJ",
        "outputId": "3472d635-a7ff-4479-c265-22f42a6c910f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1936266020090071, 0.4206103374737166, 0.9999999999999875, 1.000000000000048, 1.000000000000048, 1.000000000000048, 1.000000000000048, 0.6178456298018589, 0.6178456298018589, 0.6178456298018589, 0.6178456298018589, 0.6178456298018589, 0.6221335336862083, 0.6221335336862096, 0.616435278826629, 0.6164352788266348, 0.6164352788266655, 0.8625562199394886, 0.8625562199395426, 0.8625562199395426, 0.8625562199395426, 0.8792731213316965, 0.8792731213316975, 0.8792731213317014, 0.8792731213317014, 0.8792731213317014, 0.8792731213317014, 0.8792731213317014, 0.8372684025902045, 0.8372684025902037, 0.8372684025902037, 0.8419385006193194, 0.8466685888782737, 0.8466685888782775, 0.8466685888782783]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.99      0.90      1564\n",
            "         1.0       0.85      0.39      0.53       414\n",
            "         2.0       0.93      0.33      0.49        75\n",
            "         3.0       1.00      0.43      0.60       101\n",
            "\n",
            "    accuracy                           0.82      2154\n",
            "   macro avg       0.90      0.53      0.63      2154\n",
            "weighted avg       0.83      0.82      0.80      2154\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      1.00      0.84      1564\n",
            "         1.0       0.00      0.00      0.00       414\n",
            "         2.0       0.00      0.00      0.00        75\n",
            "         3.0       0.00      0.00      0.00       101\n",
            "\n",
            "    accuracy                           0.73      2154\n",
            "   macro avg       0.18      0.25      0.21      2154\n",
            "weighted avg       0.53      0.73      0.61      2154\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.98      0.84      1564\n",
            "         1.0       0.00      0.00      0.00       414\n",
            "         2.0       0.20      0.11      0.14        75\n",
            "         3.0       0.00      0.00      0.00       101\n",
            "\n",
            "    accuracy                           0.72      2154\n",
            "   macro avg       0.23      0.27      0.24      2154\n",
            "weighted avg       0.54      0.72      0.61      2154\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      1.00      0.84      1564\n",
            "         1.0       0.00      0.00      0.00       414\n",
            "         2.0       0.29      0.03      0.05        75\n",
            "         3.0       0.00      0.00      0.00       101\n",
            "\n",
            "    accuracy                           0.73      2154\n",
            "   macro avg       0.25      0.26      0.22      2154\n",
            "weighted avg       0.54      0.73      0.61      2154\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      1.00      0.84      1564\n",
            "         1.0       0.00      0.00      0.00       414\n",
            "         2.0       0.00      0.00      0.00        75\n",
            "         3.0       0.00      0.00      0.00       101\n",
            "\n",
            "    accuracy                           0.73      2154\n",
            "   macro avg       0.18      0.25      0.21      2154\n",
            "weighted avg       0.53      0.73      0.61      2154\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(acData)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihJjKr6jUnOz",
        "outputId": "052bde1e-a3cc-45f8-ce45-c89332b88202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[82.45, 72.61, 71.87, 72.56, 72.61]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aUbESWqrkSh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "60a5e5b6-692f-4093-ca04-2a7b498230b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "blogs = ['Learn ++', 'MLP', 'NB', 'Adaboast', 'Random Forest']\n",
        "\n",
        "\n",
        "# Creating a simple bar chart\n",
        "plt.bar(blogs, acData, edgecolor='#2c3e50',\n",
        "        color=['#c0392b', '#8e44ad', '#2980b9', '#27ae60', '#16a085', 'k', \n",
        "               'olive', 'gray', 'pink', 'maroon'], linewidth=1)\n",
        "\n",
        "plt.title('Accuracy Graph')\n",
        "plt.xlabel('Algorithm', fontsize=15)\n",
        "plt.ylabel('Accuracy', fontsize=15)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time = [0, 1, 2, 3]\n",
        "iteration = []\n",
        "for i in range(0, len(Error)):\n",
        "  iteration.append(i)\n",
        "\n",
        "plt.plot(iteration, Error, label='Decision Tree', linewidth=3, color=\"#c0392b\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Iteration No')\n",
        "plt.plot(iteration, mlpAcc, label='MLP', linewidth=3, color=\"#8e44ad\")\n",
        "plt.plot(iteration, nbAcc, label='Naive Bayes', linewidth=3, color=\"#27ae60\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "urZ_e6dRYia5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IncrementalLearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}